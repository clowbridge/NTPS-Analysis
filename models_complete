#### PREVALENCE ESTIMATES ####

#### SETUP ####

#### * Install packages ####

# install.packages("mice")
# install.packages("sandwich")
# install.packages("lmtest")
# install.packages("arm")
# install.packages("survey")
# install.packages("binom")
# install.packages("dplyr")
# install.packages("ggplot2")

library(sandwich)
library(lmtest)
#library(arm)
library(survey)
library(binom)
library(dplyr)
library(ggplot2)
library(VIM)
library(mice)

# detach("arm", unload = TRUE) # arm package interferes with dplyr select function


#### * Create colourscheme ####

colour1 <- "#80CDC1" #Light aqua
colour2 <- "#018571" # Dark teal
colour3 <- "#B2ABD2" # Light purple
colour4 <- "#5E3C99" # Dark purple
colour5 <- "#404040" # Dark grey
colour6 <- "#FFC107" # (Primary) Bright amber
colour7 <- "#BDBDBD" # Light grey

# Gradient scheme using colour6
colour6a <- "#FFF350"
colour6b <- "#FFEC47"
colour6c <- "#FFE53E"
colour6d <- "#FFDB35"
colour6e <- "#FFD22C"
colour6f <- "#FFC107"

# Updated gradient scheme using colour6
colour6a <- "#FFF350"
colour6b <- "#FFD13F"
colour6c <- "#FFAA33"
colour6d <- "#FF8026"
colour6e <- "#FF551A"
colour6f <- "#FF2A0D"



#### * Create base dataframe ####

df <- dfe %>%
  select(individual_id,
         cluster_num, 
         casedef_who, casedef_inc_trace, casedef_exc_trace,
         geoclass,
         gender, agegp5, agegp10,
         tbhx, tbhx_current,
         symptom_cough, symptom_cough_prolonged, symptom_fever, symptom_weightloss, symptom_nightsweat, symptomscreen, 
         eligible_rec, screen_pos,
         radscore, cxr_abnormality_score, cxr_finding,
         res_xpert_a, res_xpert_b, res_xpert_binary, res_culture_a, cd_culture,
         reg)


# Select desired case definition:
#   casedef_who
#   casedef_inc_trace
#   casedef_exc_trace

# Rename chosen case definition to case
df <- df %>%
  rename(case = casedef_who)



#### * Create weights for unequal cluster size (for Model 1, 2) ####

# Calculate the size of each cluster
cluster_sizes <- table(df$cluster_num)

# Calculate the total number of participants in the study
total_population <- nrow(df)

# Create a weight variable as the total study population size divided by the cluster size
df <- df %>%
  mutate(weight_cluster = total_population / cluster_sizes[as.character(cluster_num)])



#### * Create weights for group combos (for Model 3) ####

# Count N for each combination of cluster, age group, and gender - sputum eligible population (N)
eligible_count <- df %>%
  filter(screen_pos == 1) %>%
  group_by(cluster_num, agegp10, gender) %>%
  summarise(N = n(), .groups = 'drop')

# Count n for each combination of cluster, age group, and gender - survey participants (n)
participant_count <- df %>%
  filter(reg == 1) %>%
  group_by(cluster_num, agegp10, gender) %>%
  summarise(n = n(), .groups = 'drop')

# Merge the counts and calculate weights
df <- df %>%
  left_join(participant_count, by = c("cluster_num", "agegp10", "gender")) %>%
  left_join(eligible_count, by = c("cluster_num", "agegp10", "gender")) %>%
  mutate(n = ifelse(is.na(n), 0, n), # Set n to 0 if it is NA
         weight_group = ifelse(n == 0, 0, N / n)) # Set weight to 0 if n is 0

rm(eligible_count, participant_count)



#### * Combine cluster size and group weights (Model 3) ####

# Combine cluster weights with group weights
df <- df %>%
  mutate(weight_comb = weight_cluster * weight_group)


# Convert NA weights to 0
df$weight_comb[is.na(df$weight_comb)] <- 0
df$weight_group[is.na(df$weight_group)] <- 0


# Remove unused data
rm(cen, cen_hh, cen_in, clusters, dfd, int, lab, pop_agesex, pop_aldeia, rad, reg, cluster_sizes, total_population)



#### CLUSTER LEVEL ANALYSIS ####

#### Function for Crude Prevalence Calculation ####
calculate_prevalence <- function(df, filter_col, filter_val) {
  df_filtered <- df %>%
    filter(!is.na(case), !!sym(filter_col) %in% filter_val) %>%
    group_by(cluster_num, geoclass) %>%
    summarize(sum_case = sum(case), n = n(), .groups = 'drop') %>%
    mutate(rate = sum_case / n * 100000)
  
  ca_cases <- sum(df_filtered$sum_case)
  ca_mean <- mean(df_filtered$rate)
  ca_stdev <- sd(df_filtered$rate)
  ca_se <- ca_stdev / sqrt(nrow(df_filtered))
  ca_95LL <- ca_mean - (ca_se * 2)
  ca_95UL <- ca_mean + (ca_se * 2)
  
  return(c(ca_cases, ca_mean, ca_stdev, ca_se, ca_95LL, ca_95UL))
}

# Define the groups and corresponding filter columns
groups <- list(
  Urban = list(filter_col = "geoclass", filter_val = "Urban"),
  Rural = list(filter_col = "geoclass", filter_val = "Rural"),
  Male = list(filter_col = "gender", filter_val = "Male"),
  Female = list(filter_col = "gender", filter_val = "Female"),
  `15-24` = list(filter_col = "agegp10", filter_val = "15-24"),
  `25-34` = list(filter_col = "agegp10", filter_val = "25-34"),
  `35-44` = list(filter_col = "agegp10", filter_val = "35-44"),
  `45-54` = list(filter_col = "agegp10", filter_val = "45-54"),
  `55-64` = list(filter_col = "agegp10", filter_val = "55-64"),
  `65+` = list(filter_col = "agegp10", filter_val = "65+"),
  Total = list(filter_col = "geoclass", filter_val = c("Urban", "Rural"))
)

# Calculate prevalence for each group
results <- lapply(names(groups), function(group) {
  calculate_prevalence(df, groups[[group]]$filter_col, groups[[group]]$filter_val)
})

# Create the results table
results_df <- do.call(rbind, results) %>%
  as.data.frame() %>%
  setNames(c("Cases", "Prevalence", "Standard_dev", "Standard_error", "Lower_95", "Upper_95"))

# Add Group column
results_df$Group <- names(groups)

# Reorder columns
prevalence_ca <- results_df %>%
  select(Group, Cases, Standard_dev, Standard_error, Prevalence, Lower_95, Upper_95)

# Clean up the environment
rm(calculate_prevalence, groups, results, results_df)


#### * Figures #### 

# # Histogram of cluster frequency by prevalence
# p1 <- ggplot(prevalence_ca, aes(x = Prevalence)) +
#   geom_histogram(binwidth = 500, fill = colour1, color = colour5) +
#   labs(title = "Histogram of cluster frequency, by prevalence",
#        x = "Prevalence per 100,000 population",
#        y = "Frequency") +
#   theme_classic2()
# 
# p1
# 
# 
# # Prevalence rates by group
# prevalence_ca$Group <- factor(prevalence_ca$Group,
#                               levels = c("Urban", "Rural", "Male", "Female",
#                                          "15-24" , "25-34", "35-44", "45-54", "55-64", "65+",
#                                          "Total"))
# 
# p2 <- ggplot(prevalence_ca, aes(x = Group, y = Prevalence)) +
#   geom_col(aes(fill = Group), color = colour5, position = "dodge") +
#   geom_errorbar(aes(ymin = Lower_95, ymax = Upper_95), width = 0.25, position = position_dodge(0.9)) +
#   scale_fill_manual(values = c("Urban" = colour1, "Rural" = colour2,
#                                "Male" = colour3, "Female" = colour4,
#                                "15-24" = colour6a, "25-34" = colour6b, "35-44" = colour6c,
#                                "45-54" = colour6d, "55-64" = colour6e, "65+" = colour6f,
#                                "Total" = colour7)) +
#   labs(title = "Cluster-Level Analysis of Prevalence, by Group",
#        x = "Group",
#        y = "Prevalence Rate (per 100,000)") +
#   theme_classic2() +
#   guides(fill = "none")
# 
# p2
# 


#### INDIVIDUAL LEVEL ANALYSIS ####

#### ~Model 1~ ####

# Robust standard errors, no missing value imputation, no weighting, limited to participants

#### * Create dataframe ####
df1 <- df %>%
  filter(!is.na(case)) %>%
  select(cluster_num, agegp10, gender, geoclass, case, weight_cluster) %>%
  mutate(n = 1)

# Create a vector of variable names that you do not want to convert to factors
vars_to_exclude <- c("cxr_abnormality_score", "weight_cluster")

# Get the variable names to convert by excluding the specified variables
vars_to_convert <- setdiff(names(df1), vars_to_exclude)

# Convert the specified variables to factors
df1[vars_to_convert] <- lapply(df1[vars_to_convert], factor)

rm(vars_to_convert, vars_to_exclude)


#### * Create model ####

# Create survey design object with weights
design <- svydesign(id = ~cluster_num, strata = ~geoclass, weights = ~weight_cluster, data = df1)


# *** Check this line - ? no weights - Create survey design object with weights
#design <- svydesign(id = ~cluster_num, strata = ~geoclass, data = df1)



# Fit the logistic regression model with survey design
model <- svyglm(case ~ geoclass + agegp10 + gender, family = quasibinomial(link = "logit"), design = design)

# Predict probability of the outcome
df1$predicted_prob <- predict(model, design = design, type = "response")

# Aggregate prediction of overall prevalence rate
agg_predictions_tot <- data.frame(predicted_prob = mean(df1$predicted_prob))

# Calculate prevalence rate per 100,000 people
agg_predictions_tot$prevalence_rate_per_100000 <- agg_predictions_tot$predicted_prob * 100000 


#### * Robust SE for overall estimate CI ####

# Calculate robust SE for overall prevalence
vcov_matrix <- vcovHC(model, type = "HC0")
se_overall_prevalence <- sqrt(vcov_matrix["(Intercept)", "(Intercept)"]) * 100000

# Calculate the confidence limits for the prevalence rate using robust standard errors
z_value <- qnorm(0.975) # Z-value for 95% confidence interval
agg_predictions_tot$lower_ci <- agg_predictions_tot$prevalence_rate_per_100000 - (z_value * se_overall_prevalence)
agg_predictions_tot$upper_ci <- agg_predictions_tot$prevalence_rate_per_100000 + (z_value * se_overall_prevalence)


#### * Robust SE for group estimates CIs ####

# Aggregate prediction of prevalence rate by group
agg_predictions_group <- df1 %>%
  group_by(geoclass, agegp10, gender) %>%
  summarize(predicted_prob = mean(predicted_prob), .groups = 'drop')

# Calculate prevalence rate per 100,000 people
agg_predictions_group$prevalence_rate_per_100000 <- agg_predictions_group$predicted_prob * 100000

# Function to calculate standard errors for predicted probabilities
calculate_group_se <- function(model, group_data) {
  model_matrix <- model.matrix(model)
  vcov_matrix <- vcovHC(model, type = "HC0")
  
  group_se <- apply(group_data, 1, function(row) {
    # Initialize the row matrix for the group
    row_matrix <- model_matrix[1, , drop = FALSE]
    row_matrix[] <- 0
    row_matrix[, "(Intercept)"] <- 1
    
    # Set the values for each predictor in the group
    for (predictor in names(row)) {
      predictor_name <- paste(predictor, row[predictor], sep = "")
      if (predictor_name %in% colnames(row_matrix)) {
        row_matrix[, predictor_name] <- 1
      }
    }
    
    # Calculate the variance for the row
    var_pred <- row_matrix %*% vcov_matrix %*% t(row_matrix)
    
    # Return the standard error
    return(sqrt(var_pred))
  })
  
  return(group_se)
}

# Prepare group data for SE calculation
group_data <- agg_predictions_group %>%
  select(geoclass, agegp10, gender)

# Calculate standard errors for group-level predictions
agg_predictions_group$se_group <- calculate_group_se(model, group_data)

# Calculate confidence intervals for group-level predictions
agg_predictions_group <- agg_predictions_group %>%
  mutate(lower_ci = prevalence_rate_per_100000 - (z_value * se_group * 100000),
         upper_ci = prevalence_rate_per_100000 + (z_value * se_group * 100000))


#### * Results ####

# Final results
model_1_overall <- agg_predictions_tot
model_1_bygroup <- agg_predictions_group

# Clean up the environment
rm(agg_predictions_group, agg_predictions_tot, vcov_matrix, group_data, model, design, se_overall_prevalence, z_value, calculate_group_se)



#### ~Model 2~ ####

#### * Multiple imputation script ####

#### * Visualise missing data ####

# # Function to calculate percentage of missing values
# pMiss <- function(x) {sum(is.na(x)) / length(x) * 100}
# 
# # Check missing values for each column and row
# col_missing <- apply(df, 2, pMiss)
# row_missing <- apply(df, 1, pMiss)
# 
# # Display missing data pattern
# md_pattern <- md.pattern(df)
# 
# # Plot missing data pattern
# aggr_plot <- aggr(df, col = c('navyblue', 'red'), numbers = TRUE, sortVars = TRUE, labels = names(df), cex.axis = .7, gap = 3, ylab = c("Histogram of missing data", "Pattern"))
# 

#### * Imputation - step 1 ####

# Impute missing data on survey TB case status where no valid culture result

# Subset data for positive Ultra result (including those with and without valid culture result)
df1 <- df %>%
  filter(res_xpert_a != 0 | res_xpert_b != 0)

# Select variables for imputation
vars_impute_step1 <- c("res_xpert_a", "res_xpert_b", "tbhx", "tbhx_current")

# Impute missing values for "case" using selected variables
imputed_data_step1 <- mice(df1[, c("case", vars_impute_step1)], method = "logreg", m = 100, seed = 13624)

# Complete the imputation and update the "case" column
df1$case <- complete(imputed_data_step1)$case

#### * Imputation - step 2 ####

# Impute missing data on survey TB case status where no valid Ultra result

# Subset data for sputum testing eligibility including those with and without valid Ultra results
df2 <- df %>%
  filter(screen_pos == 1, is.na(res_xpert_binary))

# Combine imputed dataframe from step 1 with sputum eligible but not Ultra positive participants
df3 <- bind_rows(df1, df2)

# Select variables for imputation
vars_impute_step2 <- c("agegp5", "gender", "geoclass", "cxr_abnormality_score", 
                       "symptom_cough", "symptom_cough_prolonged", "symptom_fever", 
                       "symptom_weightloss", "symptom_nightsweat", "tbhx", "tbhx_current")

# Impute missing values for "case" using selected variables
imputed_data_step2 <- mice(df3[, c("case", vars_impute_step2)], method = "logreg", m = 100, seed = 219)

# Complete the imputation and update the "case" column
df3$case <- complete(imputed_data_step2)$case

#### * Combine data ####

# Subset data for participants not eligible for sputum examination
df4 <- df %>%
  filter(is.na(screen_pos))

# Combine with participants eligible for sputum examination
df5 <- bind_rows(df3, df4) %>%
  arrange(individual_id)

# df5 now contains all data of original df, with multiple imputation of case status completed

# Clean up the environment
rm(df1, df2, df3, df4, vars_impute_step1, vars_impute_step2, imputed_data_step1, imputed_data_step2, col_missing, row_missing, md_pattern, aggr_plot)



#### * Create dataframe ####
df1 <- df5 %>%
  filter(!is.na(case)) %>% # limits df to participants only
  select(cluster_num, agegp10, gender, geoclass, case, cxr_finding, res_xpert_a, res_xpert_b, res_culture_a, weight_group) %>%
  mutate(n = 1)

# Create a vector of variable names that you do not want to convert to factors
vars_to_exclude <- c("cxr_abnormality_score", "weight_group")

# Get the variable names to convert by excluding the specified variables
vars_to_convert <- setdiff(names(df1), vars_to_exclude)

# Convert the specified variables to factors
df1[vars_to_convert] <- lapply(df1[vars_to_convert], factor)

rm(vars_to_convert, vars_to_exclude)


#### * Create model ####

# Create survey design object with weights
design <- svydesign(id = ~cluster_num, strata = ~geoclass, weights = ~weight_group, data = df1)

# Fit the logistic regression model with survey design
model <- svyglm(case ~ geoclass + agegp10 + gender, family = quasibinomial(link = "logit"), design = design)

# Predict probability of the outcome
df1$predicted_prob <- predict(model, design = design, type = "response")

# Aggregate prediction of overall prevalence rate
agg_predictions_tot <- data.frame(predicted_prob = mean(df1$predicted_prob))

# Calculate prevalence rate per 100,000 people
agg_predictions_tot$prevalence_rate_per_100000 <- agg_predictions_tot$predicted_prob * 100000 


#### * Robust SE for overall estimate CI ####

#Calculate robust SE for overall prevalence
vcov_matrix <- vcovHC(model, type = "HC0")
se_overall_prevalence <- sqrt(vcov_matrix["(Intercept)", "(Intercept)"]) * 100000

# Calculate the confidence limits for the prevalence rate using robust standard errors
z_value <- qnorm(0.975) # Z-value for 95% confidence interval
agg_predictions_tot$lower_ci <- agg_predictions_tot$prevalence_rate_per_100000 - (z_value * se_overall_prevalence)
agg_predictions_tot$upper_ci <- agg_predictions_tot$prevalence_rate_per_100000 + (z_value * se_overall_prevalence)


#### * Robust SE for group estimates CIs ####

# Aggregate prediction of prevalence rate by group
agg_predictions_group <- df1 %>%
  group_by(geoclass, agegp10, gender) %>%
  summarize(predicted_prob = mean(predicted_prob), .groups = 'drop')

# Calculate prevalence rate per 100,000 people
agg_predictions_group$prevalence_rate_per_100000 <- agg_predictions_group$predicted_prob * 100000

# Function to calculate standard errors for predicted probabilities
calculate_group_se <- function(model, group_data) {
  model_matrix <- model.matrix(model)
  vcov_matrix <- vcovHC(model, type = "HC0")
  
  group_se <- apply(group_data, 1, function(row) {
    # Initialize the row matrix for the group
    row_matrix <- model_matrix[1, , drop = FALSE]
    row_matrix[] <- 0
    row_matrix[, "(Intercept)"] <- 1
    
    # Set the values for each predictor in the group
    for (predictor in names(row)) {
      predictor_name <- paste(predictor, row[predictor], sep = "")
      if (predictor_name %in% colnames(row_matrix)) {
        row_matrix[, predictor_name] <- 1
      }
    }
    
    # Calculate the variance for the row
    var_pred <- row_matrix %*% vcov_matrix %*% t(row_matrix)
    
    # Return the standard error
    return(sqrt(var_pred))
  })
  
  return(group_se)
}

# Prepare group data for SE calculation
group_data <- agg_predictions_group %>%
  select(geoclass, agegp10, gender)

# Calculate standard errors for group-level predictions
agg_predictions_group$se_group <- calculate_group_se(model, group_data)

# Calculate confidence intervals for group-level predictions
agg_predictions_group <- agg_predictions_group %>%
  mutate(lower_ci = prevalence_rate_per_100000 - (z_value * se_group * 100000),
         upper_ci = prevalence_rate_per_100000 + (z_value * se_group * 100000))


#### * Results ####

# Final results
model_2_overall <- agg_predictions_tot
model_2_bygroup <- agg_predictions_group

# Clean up the environment
rm(agg_predictions_group, agg_predictions_tot, vcov_matrix, group_data, model, design, se_overall_prevalence, z_value, calculate_group_se)



#### ~Model 3~ ####

#### * Create dataframe ####

# Retrieve dataset with imputation
df1 <- df5 %>%
  subset(!is.na(case)) # limits df to participants only


# Create a vector of variable names that you do not want to convert to factors
vars_to_exclude <- c("cxr_abnormality_score", "weight_cluster", "weight_group", "weight_comb")

# Get the variable names to convert by excluding the specified variables
vars_to_convert <- setdiff(names(df1), vars_to_exclude)

# Convert the specified variables to factors
df1[vars_to_convert] <- lapply(df1[vars_to_convert], factor)

rm(vars_to_convert, vars_to_exclude)

# Convert NA weights to 0
df1$weight_comb[is.na(df1$weight_comb)] <- 0
df1$weight_group[is.na(df1$weight_group)] <- 0


#### * Create model ####

# Create survey design object with weights
design <- svydesign(id = ~cluster_num, strata = ~geoclass, weights = ~weight_comb, data = df1)

# Fit the logistic regression model with survey design
model <- svyglm(case ~ geoclass + agegp10 + gender, family = quasibinomial(link = "logit"), design = design)

# Predict probability of the outcome
df1$predicted_prob <- predict(model, type = "response")


# Aggregate prediction of overall prevalence rate
agg_predictions_tot <- data.frame(predicted_prob = mean(df1$predicted_prob))

# Calculate prevalence rate per 100,000 people
agg_predictions_tot$prevalence_rate_per_100000 <- agg_predictions_tot$predicted_prob * 100000 



#### * Robust SE for overall estimate CI ####

# Calculate robust SE for overall prevalence
vcov_matrix <- vcovHC(model, type = "HC0")
se_overall_prevalence <- sqrt(vcov_matrix["(Intercept)", "(Intercept)"]) * 100000

# Calculate the confidence limits for the prevalence rate using robust standard errors
z_value <- qnorm(0.975) # Z-value for 95% confidence interval
agg_predictions_tot$lower_ci <- agg_predictions_tot$prevalence_rate_per_100000 - (z_value * se_overall_prevalence)
agg_predictions_tot$upper_ci <- agg_predictions_tot$prevalence_rate_per_100000 + (z_value * se_overall_prevalence)


#### * Robust SE for group estimates CIs ####

# Aggregate prediction of prevalence rate by group
agg_predictions_group <- df1 %>%
  group_by(geoclass, agegp10, gender) %>%
  summarize(predicted_prob = mean(predicted_prob), .groups = 'drop')

# Calculate prevalence rate per 100,000 people
agg_predictions_group$prevalence_rate_per_100000 <- agg_predictions_group$predicted_prob * 100000

# Function to calculate standard errors for predicted probabilities
calculate_group_se <- function(model, group_data) {
  model_matrix <- model.matrix(model)
  vcov_matrix <- vcovHC(model, type = "HC0")
  
  group_se <- apply(group_data, 1, function(row) {
    # Initialize the row matrix for the group
    row_matrix <- model_matrix[1, , drop = FALSE]
    row_matrix[] <- 0
    row_matrix[, "(Intercept)"] <- 1
    
    # Set the values for each predictor in the group
    for (predictor in names(row)) {
      predictor_name <- paste(predictor, row[predictor], sep = "")
      if (predictor_name %in% colnames(row_matrix)) {
        row_matrix[, predictor_name] <- 1
      }
    }
    
    # Calculate the variance for the row
    var_pred <- row_matrix %*% vcov_matrix %*% t(row_matrix)
    
    # Return the standard error
    return(sqrt(var_pred))
  })
  
  return(group_se)
}

# Prepare group data for SE calculation
group_data <- agg_predictions_group %>%
  select(geoclass, agegp10, gender)

# Calculate standard errors for group-level predictions
agg_predictions_group$se_group <- calculate_group_se(model, group_data)

# Calculate confidence intervals for group-level predictions
agg_predictions_group <- agg_predictions_group %>%
  mutate(lower_ci = prevalence_rate_per_100000 - (z_value * se_group * 100000),
         upper_ci = prevalence_rate_per_100000 + (z_value * se_group * 100000))


#### * Results ####

# Final results
model_3_overall <- agg_predictions_tot
model_3_bygroup <- agg_predictions_group

#rm(agg_predictions_group, agg_predictions_tot, vcov_matrix, group_data, model, design, se_overall_prevalence, z_value, calculate_group_se, df5, df1)


